Housing prices are a focal point for homeowners, realtors, lending institutions, insurers, investors, developers, local governments, and ordinary citizens alike. Many well-funded, data-driven attempts have been made over the past two decades. This project is an attempt to accurately predict housing prices in the Washington DC, Maryland, and Virginia (DMV) region based on the house’s number of bedrooms, number of full and half bathrooms, lot square footage, interior square footage, livable interior square footage, and state. The dataset is a subset of a private dataset from Washington Capital Partners, the biggest hard money and construction lender in the DMV region. It contains close to 7000 entries of single family homes which were either bought and or sold in the last 10 years, after 2013. This project starts with a simple linear regression with logged housing prices, and moves to the Lasso, Ridge, Bayesian Ridge, and ElasticNet models for prediction. A simple cross model loss set is used — the Mean Absolute Error, Mean Squared Error, and Root Mean Squared Error— and model inference is presented on current Zillow listings. Most importantly, this report details the wealth of possible project extensions and the possibility of an industry-standard model with the right data pipeline.
	
 This project partitions the dataset into a training and evaluation set, such that 80% of the data is first trained on the five models, and 20% is held out for evaluation against the cross model loss. The resulting cross-model losses rank the models in the following order by highest accuracy: Ridge, OLS, Bayesian Ridge, ElasticNet, and Lasso, but the loss difference is minimal. The L2 Ridge regularization often outperforms in the small feature sets. Lastly, the average R^2 value of the five regressions is .307, signaling much more work to do before this project can serve as an accurate predictor of housing prices. 
 
As emphasized in the presentation, the model’s performance severely degrades at the upper and lower boundaries of the price spectrum. First, the problem with exploding gradients when a lot size is too large, or when a house has too many bedrooms and or bathrooms. Second, the problem with predicting cheap, run down houses in improvised areas. The first problem can be addressed by adding enough high priced homes into the dataset, but the performance at the lower bounds is more complicated. First, the model would need a better encoding mechanism for location. Some ideas include the normalized average or median price of a house in a zip code, the average crime in the zip code, its walkability score and amenities score, as well as the quantified quality of its school districts and distance from the city center. Second, the model would need to differentiate between new construction and livable houses, versus broken and run down houses. This is a complicated endeavor, most likely implementable by a computer vision algorithm, but there is much limitation from the accuracy of the available data.
 
The most important factor of any robust machine learning model is the quantity, quality, and variance of the underlying dataset, and 7000 single family homes from a single construction company is inadequate for building a reliable predictor. Aside from more data, the data also needs to be taken from a wide enough variance of locations and price points, and would also need to vary in square footage, number of bedrooms and bathrooms, as well as its county level zoning requirements. If someone was strictly interested in predicting houses in this area, they could web scrape the public records for all sold houses in the area, and then cross reference them by address to an open database, such as Remine, Blacknight, or Redfin to obtain their features— bedrooms, bathrooms, lot square footage, etc. With enough supervised data cleaning, someone could confidently overfit a model to predict housing prices in the region, with percentile adjustments for changes in interest rates. However, much of this endeavor would depend on the data mining operation and the availability of cross referenced data. 
